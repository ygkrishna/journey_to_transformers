{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.8333 - loss: 0.6947\n",
      "Epoch 2/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6667 - loss: 0.7083\n",
      "Epoch 3/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5000 - loss: 0.6980\n",
      "Epoch 4/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.6947\n",
      "Epoch 5/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6667 - loss: 0.6897\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Predicted sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Custom layer to handle tf.reduce_mean\n",
    "class ReduceMeanLayer(layers.Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_mean(inputs, axis=self.axis)\n",
    "\n",
    "# Building the model\n",
    "def build_model(vocab_size, max_len, embed_dim, num_heads):\n",
    "    inputs = layers.Input(shape=(max_len,), dtype=tf.int32)  # Fixed length for now\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
    "    \n",
    "    # Multi-Head Attention layer\n",
    "    attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(embedding_layer, embedding_layer)\n",
    "    \n",
    "    # Apply ReduceMeanLayer to process the output from attention\n",
    "    attention_output = ReduceMeanLayer(axis=1)(attention_output)\n",
    "    \n",
    "    # Add dropout and dense layers\n",
    "    dropout = layers.Dropout(0.2)(attention_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(dropout)\n",
    "    \n",
    "    # Create and compile the model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Example parameters (you can adjust them as needed)\n",
    "vocab_size = 5000  # Vocabulary size\n",
    "max_len = 100  # Max sentence length (you can adjust this as needed)\n",
    "embed_dim = 128  # Embedding dimension\n",
    "num_heads = 4  # Number of attention heads\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_model(vocab_size, max_len, embed_dim, num_heads)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Sample data (for demonstration)\n",
    "texts = [\n",
    "    \"The food in the restaurant is not good\",  # Negative sentiment\n",
    "    \"I love this place, the food is amazing\",   # Positive sentiment\n",
    "    \"It was a terrible experience\",            # Negative sentiment\n",
    "    \"Best meal I've had in years\",             # Positive sentiment\n",
    "]\n",
    "\n",
    "# Sample labels (1 for positive, 0 for negative)\n",
    "labels = [0, 1, 0, 1]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Model Training\n",
    "model.fit(padded_sequences, labels, epochs=5, batch_size=2)\n",
    "\n",
    "# Test the model with a new sentence\n",
    "test_text = \"The service was horrible and the food was cold\"\n",
    "test_sequence = tokenizer.texts_to_sequences([test_text])\n",
    "test_padded_sequence = pad_sequences(test_sequence, maxlen=max_len)\n",
    "\n",
    "# Predict sentiment\n",
    "pred = model.predict(test_padded_sequence)\n",
    "print(f\"Predicted sentiment: {'Positive' if pred[0] > 0.5 else 'Negative'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
