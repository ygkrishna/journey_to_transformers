{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'and', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want', 'an']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path = 'C:/Users/xz328e/Documents/Journey_To_Transformers/imdb.npz', num_words=vocab_size)\n",
    "\n",
    "# Getting all the words from word_index dictionary\n",
    "word_idx = imdb.get_word_index(path = 'C:/Users/xz328e/Documents/Journey_To_Transformers/imdb_word_index.json')\n",
    "\n",
    "# Originally the index number of a value and not a key,\n",
    "# hence converting the index as key and the words as values\n",
    "word_idx = {i: word for word, i in word_idx.items()}\n",
    "\n",
    "# again printing the review\n",
    "print([word_idx[i] for i in x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of a review::  2697\n",
      "Min length of a review::  70\n"
     ]
    }
   ],
   "source": [
    "# Get the minimum and the maximum length of reviews\n",
    "print(\"Max length of a review:: \", len(max((x_train+x_test), key=len)))\n",
    "print(\"Min length of a review:: \", len(min((x_train+x_test), key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping a fixed length of all reviews to max 400 words\n",
    "max_words = 400\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
    "\n",
    "x_valid, y_valid = x_train[:64], y_train[:64]\n",
    "x_train_, y_train_ = x_train[64:], y_train[64:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simple_RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 400, 32)           160000    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               20608     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 180,737\n",
      "Trainable params: 180,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "390/390 [==============================] - 44s 112ms/step - loss: 0.6868 - accuracy: 0.5403 - val_loss: 0.6972 - val_accuracy: 0.4688\n",
      "Epoch 2/5\n",
      "390/390 [==============================] - 44s 114ms/step - loss: 0.6471 - accuracy: 0.6262 - val_loss: 0.6428 - val_accuracy: 0.6094\n",
      "Epoch 3/5\n",
      "390/390 [==============================] - 48s 124ms/step - loss: 0.4818 - accuracy: 0.7779 - val_loss: 0.5188 - val_accuracy: 0.7812\n",
      "Epoch 4/5\n",
      "390/390 [==============================] - 49s 125ms/step - loss: 0.5099 - accuracy: 0.7642 - val_loss: 0.5064 - val_accuracy: 0.7656\n",
      "Epoch 5/5\n",
      "390/390 [==============================] - 49s 126ms/step - loss: 0.5649 - accuracy: 0.7010 - val_loss: 0.6893 - val_accuracy: 0.5625\n",
      "\n",
      "Simple_RNN Score--->  [0.5930185317993164, 0.670960009098053]\n"
     ]
    }
   ],
   "source": [
    "# fixing every word's embedding size to be 32\n",
    "embd_len = 32\n",
    "\n",
    "# Creating a RNN model\n",
    "RNN_model = Sequential(name=\"Simple_RNN\")\n",
    "RNN_model.add(Embedding(vocab_size,\n",
    "                        embd_len,\n",
    "                        input_length=max_words))\n",
    "\n",
    "# In case of a stacked(more than one layer of RNN)\n",
    "# use return_sequences=True\n",
    "RNN_model.add(SimpleRNN(128,\n",
    "                        activation='tanh',\n",
    "                        return_sequences=False))\n",
    "RNN_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# printing model summary\n",
    "print(RNN_model.summary())\n",
    "\n",
    "# Compiling model\n",
    "RNN_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "history = RNN_model.fit(x_train_, y_train_,\n",
    "                        batch_size=64,\n",
    "                        epochs=5,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_valid, y_valid))\n",
    "\n",
    "# Printing model score on test data\n",
    "print()\n",
    "print(\"Simple_RNN Score---> \", RNN_model.evaluate(x_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanilla form of RNN gave us a Test Accuracy of 64.95%. Limitations of Simple RNN are it is unable to handle long sentences well because of its vanishing gradient problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6557 - accuracy: 0.0714\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5232 - accuracy: 0.2857\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3966 - accuracy: 0.4286\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2689 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1380 - accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0046 - accuracy: 0.5714\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8710 - accuracy: 0.5714\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7413 - accuracy: 0.5714\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.6191 - accuracy: 0.5714\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5033 - accuracy: 0.5714\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3921 - accuracy: 0.7143\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2863 - accuracy: 0.8571\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1857 - accuracy: 0.9286\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0896 - accuracy: 0.8571\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9997 - accuracy: 0.9286\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9183 - accuracy: 0.9286\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8439 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7733 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7053 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6414 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5826 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5283 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4772 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4298 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 1.00 - 0s 7ms/step - loss: 0.3876 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3512 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3194 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2900 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2625 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2372 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2148 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1952 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1777 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1617 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1468 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1333 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1210 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1100 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1003 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0915 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0836 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0763 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0698 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0639 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0587 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0540 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0459 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0424 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0392 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0315 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Predicted next word: the\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example sentence\n",
    "text = \"Bob single-handedly fought the enemy and died for his country. For his contributions, brave ______.\"\n",
    "\n",
    "# Step 1: Preprocess the text\n",
    "sentences = [text.lower()]  # Lowercase for consistency\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Tokenize the sentence\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1  # Vocabulary size (+1 for padding token)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Create input (X) and output (y) pairs for training\n",
    "sequence = sequences[0]\n",
    "X, y = [], []\n",
    "for i in range(1, len(sequence)):\n",
    "    X.append(sequence[:i])  # Input sequence up to the current word\n",
    "    y.append(sequence[i])   # The next word\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "max_len = max(len(seq) for seq in X)\n",
    "X = pad_sequences(X, maxlen=max_len, padding='pre')\n",
    "y = np.array(y)\n",
    "\n",
    "# Step 2: Define the RNN model\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    SimpleRNN(hidden_dim, return_sequences=False),  # Simple RNN layer\n",
    "    Dense(vocab_size, activation='softmax')         # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Step 4: Predict the next word\n",
    "def predict_next_word(input_text, model, tokenizer, max_len):\n",
    "    # Tokenize and pad the input text\n",
    "    sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    sequence = pad_sequences([sequence], maxlen=max_len, padding='pre')\n",
    "    \n",
    "    # Predict the next word\n",
    "    predicted_index = np.argmax(model.predict(sequence), axis=-1)[0]\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_index:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# Input for prediction\n",
    "input_text = \"bob single-handedly fought the enemy and died for his country. for his contributions, brave\"\n",
    "predicted_word = predict_next_word(input_text, model, tokenizer, max_len)\n",
    "print(f\"Predicted next word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
